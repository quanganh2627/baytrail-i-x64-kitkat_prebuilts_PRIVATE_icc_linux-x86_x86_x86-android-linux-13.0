<!DOCTYPE html
  PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3.org/TR/html4/loose.dtd">
<!-- saved from url=(0014)about:internet -->
<html xmlns:MSHelp="http://www.microsoft.com/MSHelp/" lang="en-us" xml:lang="en-us"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

<meta name="DC.Type" content="topic">
<meta name="DC.Title" content="Auto-Parallelization Overview">
<meta name="DC.subject" content="SMP systems, auto-parallelization, overview, auto-parallelizer, dataflow analysis, multithreaded programs, parallelism, parallelization, program loops, subroutines in the OpenMP* run-time library, parallel run-time, synchronization, user functions, worksharing">
<meta name="keywords" content="SMP systems, auto-parallelization, overview, auto-parallelizer, dataflow analysis, multithreaded programs, parallelism, parallelization, program loops, subroutines in the OpenMP* run-time library, parallel run-time, synchronization, user functions, worksharing">
<meta name="DC.Relation" scheme="URI" content="GUID-06B54325-1C5C-41E7-A9CD-0E3A8542DC05.htm">
<meta name="DC.Relation" scheme="URI" content="GUID-6CA8435B-71E4-479A-B631-A1143FDCBE48.htm#GUID-6CA8435B-71E4-479A-B631-A1143FDCBE48">
<meta name="DC.Relation" scheme="URI" content="GUID-29986DD5-C17F-49BB-AC9B-365B077C3909.htm#GUID-29986DD5-C17F-49BB-AC9B-365B077C3909">
<meta name="DC.Relation" scheme="URI" content="GUID-78EF5615-A3E5-4DB7-A2F2-37C9DA65C110.htm#GUID-78EF5615-A3E5-4DB7-A2F2-37C9DA65C110">
<meta name="DC.Relation" scheme="URI" content="GUID-DF76C47C-AE1D-43B8-A414-51FCD6DF13E3.htm#GUID-DF76C47C-AE1D-43B8-A414-51FCD6DF13E3">
<meta name="DC.Relation" scheme="URI" content="http://www.intel.com/software/products/softwaredocs_feedback">
<meta name="DC.Format" content="XHTML">
<meta name="DC.Identifier" content="GUID-C109A0DE-0CCF-4FE4-8A03-05DBA5D8BD77">
<meta name="DC.Language" content="en-US">
<link rel="stylesheet" type="text/css" href="intel_css_styles.css">
<title>Auto-Parallelization Overview</title>
<xml>
<MSHelp:Attr Name="DocSet" Value="Intel"></MSHelp:Attr>
<MSHelp:Attr Name="Locale" Value="kbEnglish"></MSHelp:Attr>
<MSHelp:Keyword Index="F" Term="optaps_qpar_par"></MSHelp:Keyword>
<MSHelp:Keyword Index="F" Term="intel.cpp.optaps_qpar_par"></MSHelp:Keyword>
<MSHelp:Attr Name="TopicType" Value="kbReference"></MSHelp:Attr>
</xml>
</head>
<body id="GUID-C109A0DE-0CCF-4FE4-8A03-05DBA5D8BD77">
 <!-- ==============(Start:NavScript)================= -->
 <script src="NavScript.js" language="JavaScript1.2" type="text/javascript"></script>
 <script language="JavaScript1.2" type="text/javascript">WriteNavLink(0);</script>
 <!-- ==============(End:NavScript)================= -->
<p id="header_text" style="margin-bottom : 20pt"><em>Intel&reg; C++ Compiler XE 13.1 User and Reference Guides</em></p>




<h1 class="topictitle1">Auto-Parallelization Overview</h1>




<div>
<p>The auto-parallelization feature of the Intel&reg; compiler automatically translates serial portions of the input program into equivalent multithreaded code. Automatic parallelization determines the loops that are good worksharing candidates, performs the dataflow analysis to verify correct parallel execution, and partitions the data for threaded code generation as needed in programming with OpenMP* directives. The OpenMP and auto-parallelization features provide the performance gains from shared memory on multiprocessor and dual core systems.</p>

<p>The auto-parallelizer analyzes the dataflow of the loops in the application source code and generates multithreaded code for those loops which can safely and efficiently be executed in parallel.</p>

<p>This behavior enables the potential exploitation of the parallel architecture found in symmetric multiprocessor (SMP) systems.</p>
<p>The guided auto-parallelization feature of the Intel&reg; compiler helps you locate portions in your serial code that can be parallelized further. You can invoke guidance for parallelization, vectorization, or data transformation using specified compiler options of the <span class="option">-guide</span> (Linux* OS) or <span class="option">/Qguide</span> (Windows* OS) series.
</p>

<p>Automatic parallelization frees developers from having to:</p>


<ul type="disc" id="GUID-62290E14-DFB2-4AC3-A6D7-00EF3473C30B">
  <li><p>find loops that are good worksharing candidates</p>
</li>

  <li><p>perform the dataflow analysis to verify correct parallel execution</p>
</li>

  <li><p>partition the data for threaded code generation as is needed in programming with OpenMP* directives.</p>
</li>

</ul>


<p>The parallel run-time support provides run-time features as found in OpenMP*, such as handling the details of loop iteration modification, thread scheduling, and synchronization. You can use the <span class="option">-par-runtime-control</span> (Linux* OS) or the <span class="option">/Qpar-runtime-control</span> (Windows* OS) compiler option to generate code that  performs run-time checks for loops that have symbolic loop bounds.
The loop is executed in parallel if the granularity of a loop is greater than the parallelization threshold. The parallelization threshold can be set using the <span class="option">-par-threshold</span> (Linux OS) or the<span class="option"> /Qpar-threshold</span> (Windows OS) compiler option, which sets a threshold for the auto-parallelization of loops based on the probability of profitable execution of the loop in parallel.

</p>

<p> Although OpenMP directives enable serial applications to transform into parallel applications quickly, you  must explicitly identify specific portions of your  application code that contain parallelism and add the appropriate compiler directives. Auto-parallelization, which is triggered by the <a href="GUID-29986DD5-C17F-49BB-AC9B-365B077C3909.htm#GUID-29986DD5-C17F-49BB-AC9B-365B077C3909"><span class="option">-parallel</span></a> (Linux* OS and OS X*) or <a href="GUID-29986DD5-C17F-49BB-AC9B-365B077C3909.htm#GUID-29986DD5-C17F-49BB-AC9B-365B077C3909"><span class="option">/Qparallel</span></a> (Windows* OS) option, automatically identifies those loop structures that contain parallelism. During compilation, the compiler automatically attempts to deconstruct the code sequences into separate threads for parallel processing. No other effort is needed.<div class="Note"><h3 class="NoteTipHead">Note</h3><p>In order to execute a program that uses auto-parallelization on Linux* OS or OS X* systems, you must include the <span class="option">-parallel</span> compiler option when you compile and link your program.</p>
<p>Using this option enables parallelization for both Intel&reg; microprocessors and non-Intel microprocessors. The resulting executable may get additional performance gain on Intel microprocessors than on non-Intel microprocessors. The parallelization can also be affected by certain options, such as <span class="option">/arch</span> or <span class="option">/Qx</span> (Windows) or <span class="option">-m</span> or <span class="option">-x</span> (Linux and OS X).</p>
</div>
</p>



<p>Serial code can be divided so that the code can execute concurrently on multiple threads. For example, consider the following serial code example.</p>



<div class="tablenoborder"><table cellpadding="4" summary="" id="GUID-4490DBF8-BF6E-433C-AED9-61D9930D4037" width="100%" frame="border" border="1" cellspacing="0" rules="all">

<thead align="left">
<tr valign="top">
  <th class="cellrowborder" align="left" valign="top" id="d638264e165"><p>Example 1:  Original Serial Code</p>
</th>

</tr>

</thead>


<tbody>
<tr valign="top">
  <td class="cellrowborder" valign="top" headers="d638264e165 ">

<pre>void ser(int *a, int *b, int *c)</pre>
<pre>{</pre>
<pre>  for (int i=0; i&lt;100; i++)</pre>
<pre>    a[i] = a[i] + b[i] * c[i];</pre>
<pre>}</pre>

</td>

</tr>

</tbody>
</table>
</div>


<p>The following example illustrates one method showing how the loop iteration space, shown in the previous example, might be divided to execute on two threads.</p>



<div class="tablenoborder"><table cellpadding="4" summary="" id="GUID-EF05BFEF-21A9-4155-8744-BC28B0D040F8" width="100%" frame="border" border="1" cellspacing="0" rules="all">

<thead align="left">
<tr valign="top">
  <th class="cellrowborder" align="left" valign="top" id="d638264e207"><p>Example 2: Transformed Parallel Code</p>
</th>

</tr>

</thead>


<tbody>
<tr valign="top">
  <td class="cellrowborder" valign="top" headers="d638264e207 ">

<pre>void par(int *a, int *b, int *c)</pre>
<pre>{</pre>
<pre>  int i;</pre>
<pre>  // Thread 1</pre>
<pre>  for (i=0; i&lt;50; i++)</pre>
<pre>    a[i] = a[i] + b[i] * c[i];</pre>
<pre>  // Thread 2</pre>
<pre>  for (i=50; i&lt;100; i++)</pre>
<pre>    a[i] = a[i] + b[i] * c[i];</pre>
<pre>}</pre>

</td>

</tr>

</tbody>
</table>
</div>


<div class="section" id="GUID-F5B89649-A70F-49B9-AC7C-3677F2854796"><h2 class="sectiontitle">Auto-Vectorization and Parallelization</h2>

<p>Auto-vectorization detects low-level operations in the program that can be done in parallel, and then converts the sequential program to process 2, 4, 8 or up to 16 elements in one operation, depending on the data type. In some cases auto-parallelization and vectorization can be combined for better performance results. </p>
<p>Using the <span class="option">-vec</span> (Linux* OS) or the <span class="option">/Qvec</span> (Windows* OS) option enables vectorization at default optimization levels for both Intel&reg; microprocessors and non-Intel microprocessors. Vectorization may call library routines that can result in additional performance gain on Intel microprocessors than on non-Intel microprocessors. The vectorization can also be affected by certain options, such as <span class="option">/arch</span> or <span class="option">/Qx</span> (Windows) or <span class="option">-m</span> or <span class="option">-x</span> (Linux and OS X).</p>

<p>The following example demonstrates how code can be designed to explicitly benefit from parallelization and vectorization. Assuming you compile the code shown below using <a href="GUID-29986DD5-C17F-49BB-AC9B-365B077C3909.htm#GUID-29986DD5-C17F-49BB-AC9B-365B077C3909"><span class="option">-parallel</span></a> (Linux*) or <a href="GUID-29986DD5-C17F-49BB-AC9B-365B077C3909.htm#GUID-29986DD5-C17F-49BB-AC9B-365B077C3909"><span class="option">/Qparallel</span></a> (Windows*), the compiler will parallelize the outer loop and vectorize the innermost loop.</p>



<div class="tablenoborder"><table cellpadding="4" summary="" id="GUID-DA823888-5F48-409B-A2CB-EEF99CEF2558" width="100%" frame="border" border="1" cellspacing="0" rules="all">

<thead align="left">
<tr valign="top">
  <th class="cellrowborder" align="left" valign="top" id="d638264e311"><p>Example</p>
</th>

</tr>

</thead>


<tbody>
<tr valign="top">
  <td class="cellrowborder" valign="top" headers="d638264e311 ">

<pre>#include &lt;stdio.h&gt;</pre>
<pre>#define ARR_SIZE 500 //Define array</pre>
<pre>int main()</pre>
<pre>{</pre>
<pre>  int matrix[ARR_SIZE][ARR_SIZE];</pre>
<pre>  int arrA[ARR_SIZE]={10};</pre>
<pre>  int arrB[ARR_SIZE]={30};</pre>
<pre>  int i, j;</pre>
<pre>  for(i=0;i&lt;ARR_SIZE;i++)</pre>
<pre>   {</pre>
<pre>     for(j=0;j&lt;ARR_SIZE;j++)</pre>
<pre>      {</pre>
<pre>       matrix[i][j] = arrB[i]*(arrA[i]%2+10);</pre>
<pre>      }</pre>
<pre>   }</pre>
<pre>   printf("%d\n",matrix[0][0]);</pre>
<pre>}</pre>

</td>

</tr>
</tbody>
</table>
</div>


<p>Compiling the example code with the correct options, the compiler should report results similar to the following:</p>

<p><samp class="codeph">vectorization.c(18) : (col. 6) remark: LOOP WAS VECTORIZED.</samp></p>

<p><samp class="codeph">vectorization.c(16) : (col. 3) remark: LOOP WAS AUTO-PARALLELIZED.</samp></p>



<p>With the relatively small effort of adding OpenMP* directives to existing code you can transform a sequential program into a parallel program. Options that use OpenMP* are available for both Intel&reg; and non-Intel microprocessors but these options may perform additional optimizations on Intel&reg; microprocessors than they perform on non-Intel microprocessors.  The list of major, user-visible OpenMP constructs and features that may perform differently on Intel&reg; microprocessors vs. non-Intel microprocessors include: locks (internal and user visible), the SINGLE construct, barriers (explicit and implicit), parallel loop scheduling, reductions, memory allocation, and thread affinity and binding.</p>



<p>The following example demonstrates one method of using the OpenMP pragmas within code.</p>



<div class="tablenoborder"><table cellpadding="4" summary="" id="GUID-1968E734-B999-4C83-990C-19C6125513C0" width="100%" frame="border" border="1" cellspacing="0" rules="all">

<thead align="left">
<tr valign="top">
  <th class="cellrowborder" align="left" valign="top" id="d638264e406"><p>Example</p>
</th>

</tr>

</thead>


<tbody>
<tr valign="top">
  <td class="cellrowborder" valign="top" headers="d638264e406 ">

<pre>#include &lt;stdio.h&gt;</pre>
<pre>#define ARR_SIZE 100 //Define array</pre>
<pre>void foo(int ma[][ARR_SIZE], int mb[][ARR_SIZE], int *a, int *b, int *c);</pre>
<pre>int main()</pre>
<pre>{</pre>
<pre>  int arr_a[ARR_SIZE];</pre>
<pre>  int arr_b[ARR_SIZE];</pre>
<pre>  int arr_c[ARR_SIZE];</pre>
<pre>  int i,j;</pre>
<pre>  int matrix_a[ARR_SIZE][ARR_SIZE];</pre>
<pre>  int matrix_b[ARR_SIZE][ARR_SIZE];</pre>
<pre>  #pragma omp parallel for</pre>
<pre>// Initialize the arrays and matrices.</pre>
<pre>  for(i=0;i&lt;ARR_SIZE; i++)</pre>
<pre>  {</pre>
<pre>    arr_a[i]= i;</pre>
<pre>    arr_b[i]= i;</pre>
<pre>    arr_c[i]= ARR_SIZE-i;</pre>
<pre>    for(j=0; j&lt;ARR_SIZE;j++)</pre>
<pre>    {</pre>
<pre>       matrix_a[i][j]= j;</pre>
<pre>       matrix_b[i][j]= i;</pre>
<pre>    }</pre>
<pre>  }</pre>
<pre>  foo(matrix_a, matrix_b, arr_a, arr_b, arr_c);</pre>
<pre>}</pre>
<pre>void foo(int ma[][ARR_SIZE], int mb[][ARR_SIZE], int *a, int *b, int *c)</pre>
<pre>{                                  </pre>
<pre>  int i, num, arr_x[ARR_SIZE];</pre>
<pre>  #pragma omp parallel for private(num)</pre>
<pre>// Expresses the parallelism using the OpenMP pragma: parallel for.</pre>
<pre>// The pragma guides the compiler generating multithreaded code.</pre>
<pre>// Array arr_X, mb, b, and c are shared among threads based on OpenMP</pre>
<pre>// data sharing rules. Scalar num si specifed as private</pre>
<pre>// for each thread.</pre>
<pre>  for(i=0;i&lt;ARR_SIZE;i++)</pre>
<pre>   {</pre>
<pre>     num = ma[b[i]][c[i]];</pre>
<pre>     arr_x[i]= mb[a[i]][num];</pre>
<pre>     printf("Values: %d\n", arr_x[i]); //prints values 0-ARR_SIZE-1</pre>
<pre>   }</pre>
<pre>}</pre>

</td>

</tr>
</tbody>
</table>
</div>

</div>


<p> 
	 
<div class="tablenoborder"><a name="d48e18"><!-- --></a><table cellpadding="4" summary="" id="d48e18" frame="border" border="1" cellspacing="0" rules="all"> 
		   
		  <thead align="left"> 
			 <tr> 
				<th class="cellrowborder" align="left" valign="top" width="100%" id="d638264e566"> 
				  <p id="d48e30"><a name="d48e30"><!-- --></a>Optimization Notice 
				  </p>
 
				</th>
 
			 </tr>
</thead>
 
		  <tbody> 
			 <tr> 
				<td class="bgcolor(#ccecff)" bgcolor="#ccecff" valign="top" width="100%" headers="d638264e566 "> 
				  <p>Intel's compilers may or may not optimize to the same degree for non-Intel microprocessors for optimizations that are not unique to Intel microprocessors. These optimizations include SSE2, SSE3, and SSSE3 instruction sets and other optimizations. Intel does not guarantee the availability, functionality, or effectiveness of any optimization on microprocessors not manufactured by Intel. Microprocessor-dependent optimizations in this product are intended for use with Intel microprocessors. Certain optimizations not specific to Intel microarchitecture are reserved for Intel microprocessors. Please refer to the applicable product User and Reference Guides for more information regarding the specific instruction sets covered by this notice. 
				  </p>
 
				  <p> Notice revision #20110804 
				  </p>
				  

				  </td>
 
			 </tr>
 
		  </tbody>
 
		</table>
</div>
 
	 </p>


</div>


<div class="familylinks">
<div class="parentlink"><strong>Parent topic:</strong>&nbsp;<a href="GUID-06B54325-1C5C-41E7-A9CD-0E3A8542DC05.htm">Automatic Parallelization</a></div>
</div>
<div class="See Also"><h2>See Also</h2>
<div class="linklist">
<div><a href="GUID-6CA8435B-71E4-479A-B631-A1143FDCBE48.htm#GUID-6CA8435B-71E4-479A-B631-A1143FDCBE48">Guided Auto-Parallelization Overview</a></div>
<div><a href="GUID-29986DD5-C17F-49BB-AC9B-365B077C3909.htm#GUID-29986DD5-C17F-49BB-AC9B-365B077C3909"><span class="option">-parallel</span> compiler option</a> &nbsp;</div>
<div><a href="GUID-78EF5615-A3E5-4DB7-A2F2-37C9DA65C110.htm#GUID-78EF5615-A3E5-4DB7-A2F2-37C9DA65C110"><span class="option">-par-runtime-control</span> compiler option</a> &nbsp;</div>
<div><a href="GUID-DF76C47C-AE1D-43B8-A414-51FCD6DF13E3.htm#GUID-DF76C47C-AE1D-43B8-A414-51FCD6DF13E3"><span class="option">-par-threshold</span> compiler option</a> &nbsp;</div></div><br clear="all">
<div class="docfeedback">
<div><a href="http://www.intel.com/software/products/softwaredocs_feedback" target="_blank">Submit feedback on this help topic 
		  </a></div></div></div>
</body>
</html>
